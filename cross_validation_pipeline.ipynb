{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Pipeline for TCD-SegFormer\n",
    "\n",
    "This notebook implements 5-fold cross-validation for robust model evaluation, allowing for:\n",
    "- Testing both TrueResSegformer and standard Segformer models\n",
    "- Evaluating with or without class weights\n",
    "- Comparing metrics across different folds\n",
    "- Visualizing aggregate performance\n",
    "\n",
    "The cross-validation process will train models on different data splits and evaluate their performance, providing robust statistics on model capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Import our project modules\n",
    "from config import Config\n",
    "from cross_validation import run_cross_validation\n",
    "from utils import get_logger, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger\n",
    "logger = get_logger()\n",
    "is_notebook = True  # Flag to indicate we're running in a notebook\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "logger.info(f\"Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Set up the configuration for cross-validation. You can experiment with different settings by modifying the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Base configuration \n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "config[\"output_dir\"] = f\"./outputs/cv_{timestamp}\"\n",
    "config[\"dataset_name\"] = \"restor/tcd\"  # Adjust if using a different dataset\n",
    "\n",
    "# Model configuration\n",
    "config[\"model_name\"] = \"nvidia/mit-b0\"  # Can be changed to other models like mit-b1, mit-b2, etc.\n",
    "config[\"use_true_res_segformer\"] = True  # Set to True for TrueResSegformer or False for standard Segformer\n",
    "\n",
    "# Class weights configuration\n",
    "config[\"class_weights_enabled\"] = True  # Set to True to use class weights, False otherwise\n",
    "\n",
    "# Training parameters\n",
    "config[\"num_epochs\"] = 10  # Reducing epochs for faster cross-validation, increase for production\n",
    "config[\"train_batch_size\"] = 4  # Adjust based on your GPU memory\n",
    "config[\"eval_batch_size\"] = 8  # Adjust based on your GPU memory\n",
    "config[\"learning_rate\"] = 1e-5\n",
    "config[\"apply_loss_at_original_resolution\"] = True  # Whether to upsample logits for loss at original resolution\n",
    "\n",
    "# Cross-validation parameters\n",
    "config[\"cross_validation\"][\"enabled\"] = True\n",
    "config[\"cross_validation\"][\"num_folds\"] = 5  # Standard 5-fold CV\n",
    "config[\"cross_validation\"][\"metrics_to_track\"] = [\"f1_score_class_1\", \"IoU_class_1\", \"accuracy\"]\n",
    "\n",
    "# Performance optimization\n",
    "config[\"mixed_precision\"] = True  # Enable mixed precision for faster training\n",
    "config[\"num_workers\"] = 4  # Adjust based on your CPU cores\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Save configuration for reproducibility\n",
    "config_path = os.path.join(config[\"output_dir\"], \"cross_validation_config.json\")\n",
    "config.save(config_path)\n",
    "logger.info(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "# Display key configuration settings\n",
    "print(f\"\\n===== Cross-Validation Configuration =====\\n\")\n",
    "print(f\"Model: {'TrueResSegformer' if config['use_true_res_segformer'] else 'Standard Segformer'}\")\n",
    "print(f\"Base model: {config['model_name']}\")\n",
    "print(f\"Class weights enabled: {config['class_weights_enabled']}\")\n",
    "print(f\"Loss at original resolution: {config['apply_loss_at_original_resolution']}\")\n",
    "print(f\"Number of folds: {config['cross_validation']['num_folds']}\")\n",
    "print(f\"Output directory: {config['output_dir']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Cross-Validation\n",
    "\n",
    "Execute the cross-validation process using our defined configuration. This will:\n",
    "1. Split the dataset into 5 folds\n",
    "2. Train a model on each training split\n",
    "3. Evaluate on the corresponding validation split\n",
    "4. Collect and aggregate metrics across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross validation\n",
    "logger.info(\"Starting cross-validation process...\")\n",
    "\n",
    "cv_results = run_cross_validation(\n",
    "    config=config,\n",
    "    num_folds=config[\"cross_validation\"][\"num_folds\"],\n",
    "    logger_obj=logger,\n",
    "    is_notebook=True\n",
    ")\n",
    "\n",
    "logger.info(\"Cross-validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Analyze and visualize the results of cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from the saved JSON file\n",
    "results_path = os.path.join(config[\"output_dir\"], \"cross_validation\", \"cv_results.json\")\n",
    "with open(results_path, 'r') as f:\n",
    "    saved_results = json.load(f)\n",
    "\n",
    "# Display aggregate metrics\n",
    "print(\"\\n===== Aggregate Metrics =====\\n\")\n",
    "for key, value in saved_results[\"aggregate_metrics\"].items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame with metrics across folds\n",
    "metrics_per_fold = saved_results[\"metrics_per_fold\"]\n",
    "fold_metrics_list = []\n",
    "\n",
    "for fold_idx, metrics in enumerate(metrics_per_fold):\n",
    "    metrics_dict = {\"fold\": fold_idx + 1}\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        if isinstance(metric_value, (int, float)):\n",
    "            metrics_dict[metric_name] = metric_value\n",
    "    fold_metrics_list.append(metrics_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics_list)\n",
    "metrics_df.set_index(\"fold\", inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key metrics across folds\n",
    "key_metrics = [\"f1_score_class_1\", \"IoU_class_1\", \"accuracy\", \"precision_class_1\", \"recall_class_1\"]\n",
    "available_metrics = [m for m in metrics_df.columns if any(km in m for km in key_metrics)]\n",
    "\n",
    "if available_metrics:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot bar chart\n",
    "    ax = metrics_df[available_metrics].plot(kind=\"bar\", figsize=(14, 8), rot=0)\n",
    "    plt.title(\"Metrics Across Folds\", fontsize=16)\n",
    "    plt.ylabel(\"Score\", fontsize=14)\n",
    "    plt.xlabel(\"Fold\", fontsize=14)\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.ylim(0, 1.0) \n",
    "    \n",
    "    # Add data labels on top of bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
    "        \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    metrics_chart_path = os.path.join(config[\"output_dir\"], \"metrics_by_fold.png\")\n",
    "    plt.savefig(metrics_chart_path)\n",
    "    print(f\"Metrics visualization saved to: {metrics_chart_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics trend across folds using line charts\n",
    "if available_metrics:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in available_metrics:\n",
    "        plt.plot(metrics_df.index, metrics_df[metric], marker='o', linewidth=2, label=metric)\n",
    "    \n",
    "    plt.title(\"Metrics Trend Across Folds\", fontsize=16)\n",
    "    plt.xlabel(\"Fold\", fontsize=14)\n",
    "    plt.ylabel(\"Score\", fontsize=14)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(metrics_df.index)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    trend_chart_path = os.path.join(config[\"output_dir\"], \"metrics_trend.png\")\n",
    "    plt.savefig(trend_chart_path)\n",
    "    print(f\"Trend visualization saved to: {trend_chart_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Boxplot of Metrics Distribution\n",
    "\n",
    "Visualize the distribution of each metric across all folds using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for each metric\n",
    "if available_metrics:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Convert to long format for seaborn\n",
    "    metrics_long = metrics_df[available_metrics].reset_index().melt(\n",
    "        id_vars='fold', \n",
    "        value_vars=available_metrics, \n",
    "        var_name='Metric', \n",
    "        value_name='Score'\n",
    "    )\n",
    "    \n",
    "    # Create boxplot\n",
    "    sns.boxplot(x='Metric', y='Score', data=metrics_long)\n",
    "    plt.title(\"Distribution of Metrics Across Folds\", fontsize=16)\n",
    "    plt.xlabel(\"Metric\", fontsize=14)\n",
    "    plt.ylabel(\"Score\", fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    boxplot_path = os.path.join(config[\"output_dir\"], \"metrics_boxplot.png\")\n",
    "    plt.savefig(boxplot_path)\n",
    "    print(f\"Boxplot visualization saved to: {boxplot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Performing Model\n",
    "\n",
    "Identify the best performing model across all folds based on key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing fold based on IoU or F1 score\n",
    "target_metric = \"IoU_class_1\"  # Change this to your preferred metric\n",
    "if target_metric in metrics_df.columns:\n",
    "    best_fold = metrics_df[target_metric].idxmax()\n",
    "    best_fold_metrics = metrics_df.loc[best_fold]\n",
    "    best_model_dir = saved_results[\"best_model_dirs\"][best_fold-1]  # Adjust for 0-indexing in the results\n",
    "    \n",
    "    print(f\"\\n===== Best Performing Model =====\\n\")\n",
    "    print(f\"Best fold: {best_fold}\")\n",
    "    print(f\"Best model directory: {best_model_dir}\")\n",
    "    print(f\"\\nPerformance metrics:\")\n",
    "    for metric, value in best_fold_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(f\"Target metric '{target_metric}' not found in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The cross-validation results provide robust metrics on model performance across different data splits. These results can be used to:\n",
    "\n",
    "1. Determine the most reliable model configuration\n",
    "2. Compare different architectures (TrueResSegformer vs. standard Segformer)\n",
    "3. Evaluate the impact of class weights on performance\n",
    "4. Assess model stability across different data splits\n",
    "\n",
    "For detailed analysis, review the saved metrics and visualizations in the output directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
